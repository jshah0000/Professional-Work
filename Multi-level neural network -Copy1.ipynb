{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7137aede",
   "metadata": {},
   "source": [
    "Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c021d11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_data.npy', 'test_label.npy', 'train_data.npy', 'train_label.npy']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, array, dot, random\n",
    "import os\n",
    "import random\n",
    "\n",
    "path = 'C:/Users/jaite/Uni/AI and Machine Learning/Assignment1-Dataset/Assignment1-Dataset'#file path to folder contianing data\n",
    "\n",
    "files = [file for file in os.listdir(path) if file.endswith('.npy')]#storing files from folder\n",
    "\n",
    "if len(files)!= 4: #Error will arise if number of files is not equal to 4\n",
    "    print('error')\n",
    "else:\n",
    "     print(files)\n",
    "\n",
    "x_test = np.load(os.path.join(path, files[0])) #test data\n",
    "y_test = np.load(os.path.join(path, files[1])) #test label\n",
    "x_train = np.load(os.path.join(path, files[2])) #train data\n",
    "y_train = np.load(os.path.join(path, files[3])) #train label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c47993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is normalised and then cropped for suiting to memory for the testing data\n",
    "\n",
    "test_max, test_min = x_test.max(), x_test.min() \n",
    "\n",
    "x_test = (x_test - test_min)/(test_max - test_min)\n",
    "x_test_shortened = x_test[0:1000, 0:1000] \n",
    "y_test_shortened = y_test[0:1000, 0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f207b107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data: [[0.38247855 0.47625235 0.48363969 ... 0.47529983 0.47037938 0.46245131]\n",
      " [0.66945568 0.25204303 0.56523824 ... 0.46036336 0.4500621  0.45116356]\n",
      " [0.55745004 0.26793562 0.54472877 ... 0.45806505 0.46254178 0.46046208]\n",
      " ...\n",
      " [0.40363753 0.39133204 0.48650687 ... 0.45315062 0.44216373 0.45459327]\n",
      " [0.30495653 0.46216134 0.54945989 ... 0.4542565  0.45304428 0.45269695]\n",
      " [0.6292915  0.34657816 0.51100213 ... 0.44969069 0.45191239 0.44966691]]\n",
      "testing data info: (1000, 128)\n",
      "testing labels: [[3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [6]\n",
      " [3]\n",
      " [1]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [7]\n",
      " [9]\n",
      " [8]\n",
      " [5]\n",
      " [7]\n",
      " [8]\n",
      " [6]\n",
      " [7]\n",
      " [0]\n",
      " [4]\n",
      " [9]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [6]\n",
      " [6]\n",
      " [5]\n",
      " [4]\n",
      " [5]\n",
      " [9]\n",
      " [2]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [5]\n",
      " [4]\n",
      " [6]\n",
      " [5]\n",
      " [6]\n",
      " [0]\n",
      " [9]\n",
      " [3]\n",
      " [9]\n",
      " [7]\n",
      " [6]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [7]\n",
      " [7]\n",
      " [4]\n",
      " [6]\n",
      " [7]\n",
      " [3]\n",
      " [6]\n",
      " [3]\n",
      " [6]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [7]\n",
      " [2]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [9]\n",
      " [3]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [1]\n",
      " [1]\n",
      " [7]\n",
      " [2]\n",
      " [5]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [3]\n",
      " [8]\n",
      " [6]\n",
      " [4]\n",
      " [6]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [7]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [8]\n",
      " [7]\n",
      " [4]\n",
      " [0]\n",
      " [6]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [0]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [8]\n",
      " [0]\n",
      " [8]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [1]\n",
      " [8]\n",
      " [9]\n",
      " [1]\n",
      " [2]\n",
      " [9]\n",
      " [7]\n",
      " [2]\n",
      " [9]\n",
      " [6]\n",
      " [5]\n",
      " [6]\n",
      " [3]\n",
      " [8]\n",
      " [7]\n",
      " [6]\n",
      " [2]\n",
      " [5]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [5]\n",
      " [2]\n",
      " [9]\n",
      " [5]\n",
      " [4]\n",
      " [2]\n",
      " [1]\n",
      " [6]\n",
      " [6]\n",
      " [8]\n",
      " [4]\n",
      " [8]\n",
      " [4]\n",
      " [5]\n",
      " [0]\n",
      " [9]\n",
      " [9]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [9]\n",
      " [3]\n",
      " [7]\n",
      " [5]\n",
      " [0]\n",
      " [0]\n",
      " [5]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [8]\n",
      " [6]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [5]\n",
      " [8]\n",
      " [0]\n",
      " [1]\n",
      " [7]\n",
      " [2]\n",
      " [8]\n",
      " [8]\n",
      " [7]\n",
      " [8]\n",
      " [5]\n",
      " [1]\n",
      " [8]\n",
      " [7]\n",
      " [1]\n",
      " [3]\n",
      " [0]\n",
      " [5]\n",
      " [7]\n",
      " [9]\n",
      " [7]\n",
      " [4]\n",
      " [5]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [7]\n",
      " [9]\n",
      " [8]\n",
      " [2]\n",
      " [7]\n",
      " [6]\n",
      " [9]\n",
      " [4]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [4]\n",
      " [7]\n",
      " [6]\n",
      " [5]\n",
      " [1]\n",
      " [5]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [4]\n",
      " [0]\n",
      " [5]\n",
      " [5]\n",
      " [1]\n",
      " [1]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [3]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [2]\n",
      " [5]\n",
      " [3]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [9]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [8]\n",
      " [2]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [2]\n",
      " [3]\n",
      " [6]\n",
      " [3]\n",
      " [8]\n",
      " [5]\n",
      " [0]\n",
      " [3]\n",
      " [4]\n",
      " [3]\n",
      " [9]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [0]\n",
      " [9]\n",
      " [1]\n",
      " [0]\n",
      " [7]\n",
      " [9]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [9]\n",
      " [3]\n",
      " [4]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [6]\n",
      " [3]\n",
      " [2]\n",
      " [6]\n",
      " [1]\n",
      " [8]\n",
      " [2]\n",
      " [1]\n",
      " [6]\n",
      " [8]\n",
      " [6]\n",
      " [8]\n",
      " [0]\n",
      " [4]\n",
      " [0]\n",
      " [7]\n",
      " [7]\n",
      " [5]\n",
      " [5]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [1]\n",
      " [7]\n",
      " [5]\n",
      " [4]\n",
      " [6]\n",
      " [1]\n",
      " [9]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [9]\n",
      " [3]\n",
      " [8]\n",
      " [0]\n",
      " [7]\n",
      " [2]\n",
      " [6]\n",
      " [2]\n",
      " [5]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [6]\n",
      " [8]\n",
      " [9]\n",
      " [9]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [7]\n",
      " [3]\n",
      " [2]\n",
      " [8]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [8]\n",
      " [1]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [3]\n",
      " [8]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [9]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [9]\n",
      " [0]\n",
      " [9]\n",
      " [2]\n",
      " [8]\n",
      " [7]\n",
      " [2]\n",
      " [2]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [2]\n",
      " [9]\n",
      " [6]\n",
      " [2]\n",
      " [3]\n",
      " [0]\n",
      " [3]\n",
      " [9]\n",
      " [8]\n",
      " [7]\n",
      " [8]\n",
      " [8]\n",
      " [4]\n",
      " [0]\n",
      " [1]\n",
      " [8]\n",
      " [2]\n",
      " [7]\n",
      " [9]\n",
      " [3]\n",
      " [6]\n",
      " [1]\n",
      " [9]\n",
      " [0]\n",
      " [7]\n",
      " [3]\n",
      " [7]\n",
      " [4]\n",
      " [5]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [9]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [6]\n",
      " [2]\n",
      " [5]\n",
      " [3]\n",
      " [7]\n",
      " [3]\n",
      " [7]\n",
      " [2]\n",
      " [5]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [9]\n",
      " [9]\n",
      " [5]\n",
      " [7]\n",
      " [5]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [9]\n",
      " [7]\n",
      " [3]\n",
      " [9]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [4]\n",
      " [6]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [4]\n",
      " [4]\n",
      " [3]\n",
      " [7]\n",
      " [8]\n",
      " [3]\n",
      " [7]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [7]\n",
      " [6]\n",
      " [0]\n",
      " [5]\n",
      " [4]\n",
      " [8]\n",
      " [6]\n",
      " [8]\n",
      " [5]\n",
      " [5]\n",
      " [9]\n",
      " [9]\n",
      " [9]\n",
      " [5]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [8]\n",
      " [1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [4]\n",
      " [6]\n",
      " [5]\n",
      " [4]\n",
      " [9]\n",
      " [4]\n",
      " [7]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [5]\n",
      " [3]\n",
      " [8]\n",
      " [9]\n",
      " [5]\n",
      " [8]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [0]\n",
      " [5]\n",
      " [0]\n",
      " [0]\n",
      " [4]\n",
      " [6]\n",
      " [9]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [6]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [1]\n",
      " [7]\n",
      " [6]\n",
      " [7]\n",
      " [5]\n",
      " [9]\n",
      " [1]\n",
      " [6]\n",
      " [2]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [8]\n",
      " [5]\n",
      " [9]\n",
      " [4]\n",
      " [6]\n",
      " [4]\n",
      " [3]\n",
      " [2]\n",
      " [0]\n",
      " [7]\n",
      " [6]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [9]\n",
      " [7]\n",
      " [9]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [8]\n",
      " [9]\n",
      " [7]\n",
      " [5]\n",
      " [4]\n",
      " [0]\n",
      " [8]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [3]\n",
      " [4]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [9]\n",
      " [2]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [8]\n",
      " [5]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [6]\n",
      " [4]\n",
      " [3]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [9]\n",
      " [8]\n",
      " [8]\n",
      " [5]\n",
      " [8]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [1]\n",
      " [7]\n",
      " [7]\n",
      " [1]\n",
      " [2]\n",
      " [7]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [4]\n",
      " [1]\n",
      " [2]\n",
      " [5]\n",
      " [6]\n",
      " [8]\n",
      " [7]\n",
      " [6]\n",
      " [8]\n",
      " [3]\n",
      " [0]\n",
      " [5]\n",
      " [5]\n",
      " [3]\n",
      " [0]\n",
      " [7]\n",
      " [9]\n",
      " [1]\n",
      " [3]\n",
      " [4]\n",
      " [4]\n",
      " [5]\n",
      " [3]\n",
      " [9]\n",
      " [5]\n",
      " [6]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [4]\n",
      " [7]\n",
      " [6]\n",
      " [3]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [3]\n",
      " [6]\n",
      " [3]\n",
      " [2]\n",
      " [0]\n",
      " [3]\n",
      " [1]\n",
      " [0]\n",
      " [5]\n",
      " [9]\n",
      " [6]\n",
      " [4]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [9]\n",
      " [6]\n",
      " [3]\n",
      " [0]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [3]\n",
      " [8]\n",
      " [2]\n",
      " [7]\n",
      " [5]\n",
      " [7]\n",
      " [2]\n",
      " [4]\n",
      " [8]\n",
      " [7]\n",
      " [4]\n",
      " [2]\n",
      " [9]\n",
      " [8]\n",
      " [8]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [7]\n",
      " [4]\n",
      " [3]\n",
      " [3]\n",
      " [8]\n",
      " [4]\n",
      " [9]\n",
      " [4]\n",
      " [8]\n",
      " [8]\n",
      " [1]\n",
      " [8]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [5]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [9]\n",
      " [7]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [5]\n",
      " [9]\n",
      " [2]\n",
      " [9]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [6]\n",
      " [8]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [5]\n",
      " [4]\n",
      " [6]\n",
      " [1]\n",
      " [6]\n",
      " [4]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [6]\n",
      " [0]\n",
      " [5]\n",
      " [9]\n",
      " [1]\n",
      " [7]\n",
      " [6]\n",
      " [7]\n",
      " [0]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [3]\n",
      " [0]\n",
      " [3]\n",
      " [4]\n",
      " [7]\n",
      " [7]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [2]\n",
      " [7]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [4]\n",
      " [4]\n",
      " [8]\n",
      " [4]\n",
      " [7]\n",
      " [7]\n",
      " [5]\n",
      " [3]\n",
      " [7]\n",
      " [2]\n",
      " [0]\n",
      " [8]\n",
      " [9]\n",
      " [5]\n",
      " [8]\n",
      " [3]\n",
      " [6]\n",
      " [2]\n",
      " [0]\n",
      " [8]\n",
      " [7]\n",
      " [3]\n",
      " [7]\n",
      " [6]\n",
      " [5]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [5]\n",
      " [4]\n",
      " [1]\n",
      " [2]\n",
      " [9]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [4]\n",
      " [7]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [7]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [8]\n",
      " [4]\n",
      " [6]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [7]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [2]\n",
      " [3]\n",
      " [8]\n",
      " [4]\n",
      " [2]\n",
      " [3]\n",
      " [7]\n",
      " [8]\n",
      " [4]\n",
      " [3]\n",
      " [0]\n",
      " [9]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [4]\n",
      " [4]\n",
      " [6]\n",
      " [7]\n",
      " [6]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [7]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [6]\n",
      " [6]\n",
      " [5]\n",
      " [8]\n",
      " [7]\n",
      " [1]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [5]\n",
      " [3]\n",
      " [0]\n",
      " [4]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [6]\n",
      " [9]\n",
      " [9]\n",
      " [9]\n",
      " [5]\n",
      " [5]\n",
      " [8]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [4]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [7]\n",
      " [2]\n",
      " [2]\n",
      " [5]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [1]\n",
      " [7]\n",
      " [4]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [8]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [3]\n",
      " [7]\n",
      " [8]\n",
      " [9]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [6]\n",
      " [6]\n",
      " [6]\n",
      " [9]\n",
      " [1]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [2]\n",
      " [1]\n",
      " [7]\n",
      " [0]\n",
      " [6]\n",
      " [8]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [4]\n",
      " [7]\n",
      " [8]\n",
      " [3]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [5]\n",
      " [8]\n",
      " [4]\n",
      " [6]\n",
      " [3]\n",
      " [8]\n",
      " [1]\n",
      " [3]\n",
      " [8]]\n",
      "testing labels info: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "#confirming the matching of the data points for the testing data\n",
    "print('testing data:',x_test_shortened)\n",
    "print('testing data info:',x_test_shortened.shape)\n",
    "print('testing labels:', y_test_shortened)\n",
    "print('testing labels info:', y_test_shortened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf4c2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: [[0.34725272 0.53359344 0.50854417 ... 0.48257548 0.47476031 0.48899853]\n",
      " [0.49483041 0.45850692 0.60042901 ... 0.47877645 0.47757412 0.47199495]\n",
      " [0.63566509 0.24273347 0.4216946  ... 0.47819259 0.47063253 0.48165611]\n",
      " ...\n",
      " [0.40126729 0.3319683  0.47635369 ... 0.47936753 0.48299721 0.48924297]\n",
      " [0.67647365 0.38950872 0.52205819 ... 0.48903333 0.4688173  0.49010877]\n",
      " [0.40773303 0.45405745 0.29205533 ... 0.48214306 0.48155884 0.47323528]]\n",
      "training data info: (1000, 128)\n",
      "training labels: [[6]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [3]\n",
      " [4]\n",
      " [7]\n",
      " [7]\n",
      " [2]\n",
      " [9]\n",
      " [9]\n",
      " [9]\n",
      " [3]\n",
      " [2]\n",
      " [6]\n",
      " [4]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [6]\n",
      " [3]\n",
      " [5]\n",
      " [4]\n",
      " [0]\n",
      " [0]\n",
      " [9]\n",
      " [1]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [3]\n",
      " [7]\n",
      " [3]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [2]\n",
      " [7]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [7]\n",
      " [9]\n",
      " [2]\n",
      " [2]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [8]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [9]\n",
      " [7]\n",
      " [8]\n",
      " [5]\n",
      " [9]\n",
      " [6]\n",
      " [7]\n",
      " [3]\n",
      " [1]\n",
      " [9]\n",
      " [0]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [5]\n",
      " [4]\n",
      " [5]\n",
      " [7]\n",
      " [7]\n",
      " [4]\n",
      " [7]\n",
      " [9]\n",
      " [4]\n",
      " [2]\n",
      " [3]\n",
      " [8]\n",
      " [0]\n",
      " [1]\n",
      " [6]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [8]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [8]\n",
      " [5]\n",
      " [2]\n",
      " [9]\n",
      " [9]\n",
      " [8]\n",
      " [1]\n",
      " [7]\n",
      " [7]\n",
      " [0]\n",
      " [0]\n",
      " [6]\n",
      " [9]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [9]\n",
      " [2]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [9]\n",
      " [5]\n",
      " [0]\n",
      " [4]\n",
      " [7]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [8]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [8]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [6]\n",
      " [2]\n",
      " [4]\n",
      " [9]\n",
      " [9]\n",
      " [5]\n",
      " [4]\n",
      " [3]\n",
      " [6]\n",
      " [7]\n",
      " [4]\n",
      " [6]\n",
      " [8]\n",
      " [5]\n",
      " [5]\n",
      " [4]\n",
      " [3]\n",
      " [1]\n",
      " [8]\n",
      " [4]\n",
      " [7]\n",
      " [6]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [8]\n",
      " [2]\n",
      " [7]\n",
      " [5]\n",
      " [3]\n",
      " [4]\n",
      " [1]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [4]\n",
      " [7]\n",
      " [5]\n",
      " [5]\n",
      " [1]\n",
      " [0]\n",
      " [9]\n",
      " [6]\n",
      " [9]\n",
      " [0]\n",
      " [8]\n",
      " [7]\n",
      " [8]\n",
      " [8]\n",
      " [2]\n",
      " [5]\n",
      " [2]\n",
      " [3]\n",
      " [5]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [9]\n",
      " [3]\n",
      " [6]\n",
      " [9]\n",
      " [1]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [8]\n",
      " [5]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [6]\n",
      " [9]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [3]\n",
      " [7]\n",
      " [6]\n",
      " [6]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [7]\n",
      " [1]\n",
      " [5]\n",
      " [8]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [8]\n",
      " [6]\n",
      " [8]\n",
      " [4]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [3]\n",
      " [8]\n",
      " [3]\n",
      " [4]\n",
      " [1]\n",
      " [7]\n",
      " [1]\n",
      " [3]\n",
      " [8]\n",
      " [5]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [3]\n",
      " [7]\n",
      " [4]\n",
      " [9]\n",
      " [9]\n",
      " [2]\n",
      " [4]\n",
      " [9]\n",
      " [9]\n",
      " [1]\n",
      " [0]\n",
      " [5]\n",
      " [9]\n",
      " [0]\n",
      " [8]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [5]\n",
      " [6]\n",
      " [3]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [8]\n",
      " [6]\n",
      " [0]\n",
      " [7]\n",
      " [9]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [4]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [5]\n",
      " [9]\n",
      " [9]\n",
      " [0]\n",
      " [8]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [3]\n",
      " [3]\n",
      " [9]\n",
      " [0]\n",
      " [7]\n",
      " [9]\n",
      " [7]\n",
      " [7]\n",
      " [9]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [6]\n",
      " [6]\n",
      " [8]\n",
      " [7]\n",
      " [1]\n",
      " [3]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [4]\n",
      " [5]\n",
      " [7]\n",
      " [5]\n",
      " [9]\n",
      " [0]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [4]\n",
      " [4]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [0]\n",
      " [8]\n",
      " [1]\n",
      " [6]\n",
      " [2]\n",
      " [9]\n",
      " [2]\n",
      " [5]\n",
      " [9]\n",
      " [6]\n",
      " [7]\n",
      " [4]\n",
      " [1]\n",
      " [8]\n",
      " [7]\n",
      " [3]\n",
      " [6]\n",
      " [9]\n",
      " [3]\n",
      " [0]\n",
      " [4]\n",
      " [0]\n",
      " [5]\n",
      " [1]\n",
      " [0]\n",
      " [3]\n",
      " [4]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [7]\n",
      " [2]\n",
      " [3]\n",
      " [9]\n",
      " [7]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [1]\n",
      " [7]\n",
      " [3]\n",
      " [1]\n",
      " [8]\n",
      " [4]\n",
      " [4]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [9]\n",
      " [0]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [2]\n",
      " [7]\n",
      " [7]\n",
      " [4]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [8]\n",
      " [9]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [2]\n",
      " [5]\n",
      " [2]\n",
      " [5]\n",
      " [1]\n",
      " [9]\n",
      " [4]\n",
      " [8]\n",
      " [5]\n",
      " [1]\n",
      " [7]\n",
      " [4]\n",
      " [4]\n",
      " [0]\n",
      " [6]\n",
      " [9]\n",
      " [0]\n",
      " [7]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [9]\n",
      " [3]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [6]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [8]\n",
      " [0]\n",
      " [4]\n",
      " [8]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [2]\n",
      " [6]\n",
      " [8]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [7]\n",
      " [7]\n",
      " [5]\n",
      " [9]\n",
      " [6]\n",
      " [2]\n",
      " [8]\n",
      " [3]\n",
      " [4]\n",
      " [7]\n",
      " [3]\n",
      " [9]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [4]\n",
      " [8]\n",
      " [1]\n",
      " [8]\n",
      " [6]\n",
      " [4]\n",
      " [4]\n",
      " [5]\n",
      " [7]\n",
      " [1]\n",
      " [3]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [1]\n",
      " [7]\n",
      " [5]\n",
      " [8]\n",
      " [2]\n",
      " [8]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [2]\n",
      " [9]\n",
      " [9]\n",
      " [2]\n",
      " [7]\n",
      " [5]\n",
      " [7]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [4]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [1]\n",
      " [6]\n",
      " [4]\n",
      " [0]\n",
      " [4]\n",
      " [6]\n",
      " [9]\n",
      " [7]\n",
      " [6]\n",
      " [2]\n",
      " [5]\n",
      " [5]\n",
      " [1]\n",
      " [7]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [9]\n",
      " [5]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [3]\n",
      " [4]\n",
      " [3]\n",
      " [7]\n",
      " [6]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [6]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [8]\n",
      " [4]\n",
      " [0]\n",
      " [1]\n",
      " [8]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [7]\n",
      " [6]\n",
      " [4]\n",
      " [5]\n",
      " [8]\n",
      " [7]\n",
      " [1]\n",
      " [9]\n",
      " [1]\n",
      " [9]\n",
      " [8]\n",
      " [4]\n",
      " [7]\n",
      " [3]\n",
      " [8]\n",
      " [8]\n",
      " [2]\n",
      " [6]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [6]\n",
      " [8]\n",
      " [1]\n",
      " [9]\n",
      " [7]\n",
      " [8]\n",
      " [3]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [8]\n",
      " [8]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [5]\n",
      " [0]\n",
      " [8]\n",
      " [8]\n",
      " [7]\n",
      " [9]\n",
      " [9]\n",
      " [0]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [4]\n",
      " [4]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [0]\n",
      " [8]\n",
      " [0]\n",
      " [3]\n",
      " [2]\n",
      " [8]\n",
      " [4]\n",
      " [6]\n",
      " [9]\n",
      " [9]\n",
      " [7]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [6]\n",
      " [7]\n",
      " [4]\n",
      " [9]\n",
      " [1]\n",
      " [6]\n",
      " [2]\n",
      " [7]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [6]\n",
      " [7]\n",
      " [5]\n",
      " [7]\n",
      " [6]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]\n",
      " [4]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [9]\n",
      " [4]\n",
      " [9]\n",
      " [6]\n",
      " [9]\n",
      " [4]\n",
      " [5]\n",
      " [7]\n",
      " [9]\n",
      " [2]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [5]\n",
      " [6]\n",
      " [9]\n",
      " [3]\n",
      " [3]\n",
      " [5]\n",
      " [0]\n",
      " [7]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [4]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [5]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [3]\n",
      " [9]\n",
      " [8]\n",
      " [4]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [6]\n",
      " [4]\n",
      " [4]\n",
      " [0]\n",
      " [1]\n",
      " [8]\n",
      " [8]\n",
      " [3]\n",
      " [6]\n",
      " [9]\n",
      " [6]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [2]\n",
      " [4]\n",
      " [5]\n",
      " [7]\n",
      " [6]\n",
      " [5]\n",
      " [3]\n",
      " [0]\n",
      " [5]\n",
      " [0]\n",
      " [5]\n",
      " [0]\n",
      " [8]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [3]\n",
      " [8]\n",
      " [2]\n",
      " [1]\n",
      " [7]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [5]\n",
      " [0]\n",
      " [1]\n",
      " [7]\n",
      " [6]\n",
      " [9]\n",
      " [0]\n",
      " [4]\n",
      " [7]\n",
      " [7]\n",
      " [1]\n",
      " [5]\n",
      " [9]\n",
      " [4]\n",
      " [0]\n",
      " [8]\n",
      " [5]\n",
      " [9]\n",
      " [9]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [8]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [8]\n",
      " [2]\n",
      " [2]\n",
      " [4]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [5]\n",
      " [3]\n",
      " [8]\n",
      " [2]\n",
      " [3]\n",
      " [7]\n",
      " [2]\n",
      " [9]\n",
      " [3]\n",
      " [8]\n",
      " [7]\n",
      " [8]\n",
      " [2]\n",
      " [7]\n",
      " [9]\n",
      " [0]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [6]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [5]\n",
      " [1]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [1]\n",
      " [7]\n",
      " [7]\n",
      " [8]\n",
      " [2]\n",
      " [9]\n",
      " [2]\n",
      " [2]\n",
      " [4]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [6]\n",
      " [6]\n",
      " [5]\n",
      " [1]\n",
      " [1]\n",
      " [7]\n",
      " [0]\n",
      " [4]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [7]\n",
      " [8]\n",
      " [8]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [3]\n",
      " [0]\n",
      " [9]\n",
      " [4]\n",
      " [3]\n",
      " [8]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [9]\n",
      " [3]\n",
      " [1]\n",
      " [8]\n",
      " [9]\n",
      " [3]\n",
      " [9]\n",
      " [9]\n",
      " [2]\n",
      " [9]\n",
      " [4]\n",
      " [8]\n",
      " [2]\n",
      " [9]\n",
      " [8]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [8]\n",
      " [7]\n",
      " [6]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [6]\n",
      " [4]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [5]\n",
      " [8]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [7]\n",
      " [6]\n",
      " [9]\n",
      " [7]\n",
      " [1]\n",
      " [5]\n",
      " [5]\n",
      " [6]\n",
      " [6]\n",
      " [3]\n",
      " [6]\n",
      " [2]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [5]\n",
      " [6]\n",
      " [4]\n",
      " [6]\n",
      " [5]\n",
      " [2]\n",
      " [4]\n",
      " [6]\n",
      " [1]\n",
      " [6]\n",
      " [0]\n",
      " [4]\n",
      " [0]\n",
      " [3]\n",
      " [1]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [4]\n",
      " [1]\n",
      " [7]\n",
      " [3]\n",
      " [9]\n",
      " [4]\n",
      " [7]\n",
      " [9]\n",
      " [7]\n",
      " [3]\n",
      " [7]\n",
      " [2]\n",
      " [8]\n",
      " [4]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [4]\n",
      " [8]\n",
      " [7]\n",
      " [3]\n",
      " [9]\n",
      " [8]\n",
      " [7]\n",
      " [7]\n",
      " [0]\n",
      " [2]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [0]\n",
      " [5]\n",
      " [6]\n",
      " [2]\n",
      " [8]\n",
      " [5]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [5]\n",
      " [7]\n",
      " [3]\n",
      " [5]\n",
      " [1]\n",
      " [3]\n",
      " [5]]\n",
      "training labels info: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "#same process applied to testing dataset\n",
    "\n",
    "train_max, train_min = x_train.max(), x_train.min()\n",
    "\n",
    "x_train = (x_train-train_min)/(train_max-train_min)\n",
    "\n",
    "x_train_shortened = x_train[0:1000,0:1000]\n",
    "y_train_shortened = y_train[0:1000,0:1000]\n",
    "print('training data:', x_train_shortened)\n",
    "print('training data info:', x_train_shortened.shape)\n",
    "print('training labels:', y_train_shortened)\n",
    "print('training labels info:', y_train_shortened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb83ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels for test: [0 1 2 3 4 5 6 7 8 9]\n",
      "labels for training: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels for test:\",np.unique(y_test_shortened))\n",
    "print(\"labels for training:\", np.unique(y_train_shortened))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1666b",
   "metadata": {},
   "source": [
    "Section 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ac1b2",
   "metadata": {},
   "source": [
    "In this code snippet the neural network consists of 4 layers: the input layer, the two hidden layers and the final output layer.\n",
    "The activation function associated with the input and two hidden layers is the ReLU function however, for the output layer, the softmax cross-entropy function is used. The weights are initialised randomly using the Kaiming or He initialisation to take into account the non-linearity of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6514be28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no nan present layer weights\n",
      "no nan present layer weights\n",
      "no nan present layer weights\n",
      "no nan present layer weights\n",
      "Input Layer, 128 neurons:\n",
      "[[0.20228409 0.03788016 0.06170722 ... 0.04239158 0.17851279 0.14186024]\n",
      " [0.0662963  0.2372929  0.00224987 ... 0.12602578 0.14317393 0.01192958]\n",
      " [0.17959826 0.24066175 0.22648745 ... 0.14737648 0.0515794  0.24322277]\n",
      " ...\n",
      " [0.08609032 0.04878823 0.04178161 ... 0.08114466 0.14398103 0.04728632]\n",
      " [0.16517267 0.0571333  0.11995118 ... 0.21256906 0.19798698 0.1977356 ]\n",
      " [0.22855396 0.04709697 0.20192139 ... 0.08982199 0.19437528 0.02515634]]\n",
      "Hidden Layer 1, 64 neurons:\n",
      "[[0.14398331 0.03843232 0.14641213 ... 0.04085896 0.17641567 0.08207941]\n",
      " [0.18965722 0.1074977  0.15689354 ... 0.16721359 0.24035438 0.05310932]\n",
      " [0.24089148 0.08059437 0.00726509 ... 0.05275385 0.15893524 0.09752226]\n",
      " ...\n",
      " [0.1079148  0.19170164 0.11912579 ... 0.18428672 0.09861976 0.17223563]\n",
      " [0.07249806 0.19721808 0.05356178 ... 0.10347944 0.03299922 0.16208777]\n",
      " [0.1364714  0.11611272 0.04384275 ... 0.04156671 0.07470541 0.18695346]]\n",
      "Hidden Layer 2, 32 neurons:\n",
      "[[0.16879477 0.26099436 0.06034536 ... 0.19075856 0.23953843 0.13132674]\n",
      " [0.06144387 0.00829542 0.29212628 ... 0.25832691 0.32738664 0.27370326]\n",
      " [0.32923512 0.15505754 0.12494995 ... 0.30732741 0.01314431 0.36399645]\n",
      " ...\n",
      " [0.28908917 0.29855666 0.31780441 ... 0.14064491 0.26861    0.31898924]\n",
      " [0.32052869 0.22843727 0.18040166 ... 0.28949697 0.30349853 0.35045272]\n",
      " [0.07696112 0.26260903 0.13503821 ... 0.21855662 0.10401124 0.3408755 ]]\n",
      "Output Layer, 10 neurons:\n",
      "[[ 2.99792145e-01  1.31733292e-01  2.81054787e-01  3.34685416e-01\n",
      "   4.53560760e-02  5.93832582e-01  8.97840142e-02 -1.90337536e-02\n",
      "   2.93978635e-02  2.24819589e-01]\n",
      " [ 3.27235854e-01  2.12206830e-01  7.47511725e-02  2.03514324e-01\n",
      "   3.74062710e-01  3.96278832e-01  7.27749716e-02  2.24045717e-01\n",
      "   2.47980405e-01  3.84712416e-01]\n",
      " [ 1.51857011e-01  3.11944284e-01  1.30917219e-01  4.45796459e-01\n",
      "   3.85192227e-01  4.61897600e-01  2.36154400e-01  4.28530499e-01\n",
      "   4.35312341e-01  3.62110589e-01]\n",
      " [ 8.26181633e-02  1.19017212e-01  1.22757836e-01  5.45802719e-01\n",
      "   4.17065046e-01  2.99600720e-01  1.16799080e-01  3.64768291e-01\n",
      "   8.84231539e-02  1.94394031e-01]\n",
      " [ 4.61748778e-01  2.73895884e-01  1.58351080e-01  3.97460451e-01\n",
      "   5.05091593e-01  4.98381298e-01  2.41501402e-01  4.32004426e-01\n",
      "   1.43647554e-01  1.53971620e-01]\n",
      " [ 2.66906901e-01 -9.79455724e-02  5.10433249e-01  3.17665189e-01\n",
      "   1.71121427e-01  4.43394278e-01 -3.61503888e-02  1.33307326e-01\n",
      "   4.03709032e-02  4.41493705e-01]\n",
      " [ 1.38663071e-01  3.05868999e-01  1.11122656e-01  2.58047136e-01\n",
      "   3.80753673e-01  6.12074762e-01  2.42077406e-01 -2.60169898e-03\n",
      "   2.81002638e-01  5.91512217e-02]\n",
      " [ 1.67833038e-01 -3.49801634e-02  1.96778514e-01  1.24229955e-01\n",
      "   4.58235867e-02  2.71342141e-01  3.30757406e-01  3.01647703e-01\n",
      "   3.20853440e-01  2.95633888e-02]\n",
      " [ 1.27075247e-01 -1.31957483e-02  4.02518964e-01  5.45633561e-01\n",
      "   3.69306779e-01  2.20571377e-01  3.66429898e-01  1.08791525e-01\n",
      "   3.17081853e-01  1.69707115e-01]\n",
      " [-1.41945182e-02  5.61662065e-02  4.00293261e-01  3.54454561e-01\n",
      "   3.17761100e-01  6.32119113e-01 -9.11900590e-02  4.21307853e-01\n",
      "   5.63756691e-02  4.55755670e-01]\n",
      " [ 4.64131225e-01 -1.32003516e-02  9.14406547e-02  3.89085641e-01\n",
      "   4.10518074e-01  6.28643063e-01  3.60430788e-01  3.30480555e-01\n",
      "   3.63461431e-01  4.51100761e-01]\n",
      " [ 2.65476431e-01 -4.94510328e-02  2.06402885e-01  3.98207774e-01\n",
      "   4.21462480e-01  3.36531771e-01  1.66759766e-01  1.90095853e-01\n",
      "   4.02134110e-01  3.29529708e-01]\n",
      " [ 6.93360870e-02  3.15432514e-02  6.65924718e-02  3.13130577e-01\n",
      "   2.06935622e-02  4.94015274e-01  3.43406407e-01  2.35822293e-01\n",
      "   3.32006326e-01  2.41205160e-01]\n",
      " [ 3.99257705e-01  1.89321790e-01  4.72036082e-01  4.24494460e-01\n",
      "   1.70969403e-01  4.47621287e-01 -2.48120973e-02  4.70880120e-01\n",
      "   8.34069507e-02  4.30316840e-01]\n",
      " [ 3.07114854e-01  1.12186758e-01  4.02084239e-01  3.31998714e-01\n",
      "   8.35252389e-02  3.80215373e-01  2.51951130e-01  4.24378374e-01\n",
      "   1.57527336e-01  1.33485608e-02]\n",
      " [ 3.79166017e-02  2.92371102e-01  4.73841544e-01  2.44882520e-01\n",
      "   3.38353255e-01  3.75074762e-01  4.02029656e-01  1.32761801e-01\n",
      "   1.33720957e-01  3.20087813e-01]\n",
      " [ 2.06594938e-01  2.17242025e-01  4.06131030e-01  4.05625213e-01\n",
      "   2.41629093e-01  7.05774605e-01 -1.08959078e-02  3.32210856e-01\n",
      "   1.15754188e-01  4.30245068e-01]\n",
      " [ 4.30175342e-01 -1.08109947e-01  4.43344920e-01  3.75223618e-01\n",
      "   3.88880878e-01  4.30766038e-01  2.17375613e-01  3.46860532e-01\n",
      "   2.56788534e-01  1.03812579e-01]\n",
      " [ 2.66864347e-01 -5.55120130e-02  3.10370918e-01  5.60205804e-01\n",
      "   1.60783118e-01  2.93029085e-01 -3.34353926e-02  4.68916922e-01\n",
      "   3.99534672e-01  2.99122812e-01]\n",
      " [ 2.09409432e-01  1.86900526e-01  4.55639647e-02  4.82520685e-01\n",
      "   4.48973629e-01  4.22580157e-01  1.29580226e-01  4.65202580e-01\n",
      "   8.69997657e-02  3.49828261e-01]\n",
      " [ 1.00690416e-01  1.61294504e-01  7.80164713e-02  1.98549556e-01\n",
      "   2.49819886e-02  2.33801502e-01 -2.33322808e-02  2.45332695e-01\n",
      "   4.16725345e-01  1.04616752e-02]\n",
      " [ 1.39864794e-01  1.38707824e-01  2.07152023e-01  2.31046517e-01\n",
      "   3.96196726e-01  6.63030515e-01  1.09028535e-01  3.52434095e-01\n",
      "   3.99894190e-01  3.38194517e-01]\n",
      " [ 2.69542780e-01 -2.04687302e-02  2.92297107e-01  6.01049849e-01\n",
      "   3.25096025e-01  2.35654915e-01 -5.52342188e-02  2.46759341e-01\n",
      "   2.83294240e-01  3.87389612e-01]\n",
      " [ 3.17956511e-01  2.18333452e-02  3.09187798e-01  2.35813064e-01\n",
      "   4.95836774e-01  3.33305262e-01 -3.77067771e-03  3.84208134e-01\n",
      "   4.92829891e-01  6.42236144e-02]\n",
      " [ 2.05963598e-01 -8.91700543e-02  3.26715385e-01  4.04382696e-01\n",
      "   2.47529911e-01  6.26104636e-01  2.89238456e-01  2.84093543e-01\n",
      "   4.83597750e-01  3.48818216e-01]\n",
      " [ 3.24823145e-01  2.02442803e-01  2.71139507e-01  1.32439489e-01\n",
      "   1.92906580e-02  2.81233280e-01  2.17563170e-02  2.05165722e-01\n",
      "   1.77649411e-01  1.02828642e-01]\n",
      " [ 3.40908417e-01  6.81757858e-03  4.27307284e-02  1.06522038e-01\n",
      "   3.65674623e-01  3.82819464e-01  1.95123512e-01  4.58070023e-01\n",
      "   4.03260569e-01  3.55379088e-01]\n",
      " [ 3.57197895e-01  2.70192225e-03  6.77023900e-02  3.91599225e-01\n",
      "   1.95768630e-01  6.10767471e-01  3.99125165e-02  2.97725144e-01\n",
      "   4.70907268e-01  3.06347048e-01]\n",
      " [ 9.10484141e-02 -1.07695808e-01  1.80360078e-01  3.12009659e-01\n",
      "   3.36132510e-01  5.64011616e-01 -5.58155928e-02  3.93982002e-01\n",
      "   2.42222004e-01  2.95530771e-01]\n",
      " [ 3.97072639e-01  1.70970904e-01  3.27269230e-02  3.26684560e-01\n",
      "   2.59581521e-01  6.15172681e-01  3.75830695e-02  3.86087673e-01\n",
      "   2.26120113e-01  1.13510842e-01]\n",
      " [ 4.45606062e-01  2.33949087e-01  4.69539727e-01  1.67561347e-01\n",
      "   8.63416881e-02  2.32312701e-01  5.70549855e-02  4.66294513e-01\n",
      "   2.75516792e-01  1.78537655e-01]\n",
      " [ 4.13809749e-04 -7.99518962e-02  8.83997480e-02  3.71438577e-01\n",
      "   1.28045350e-01  6.27297344e-01  7.92982487e-02  5.97921715e-02\n",
      "   2.53747998e-01  7.09193192e-02]]\n",
      "weights after training:\n",
      "Input Layer, 128 neurons:\n",
      "[[0.20228409 0.03788016 0.06170722 ... 0.04239158 0.17851279 0.14186024]\n",
      " [0.0662963  0.2372929  0.00224987 ... 0.12602578 0.14317393 0.01192958]\n",
      " [0.17959826 0.24066175 0.22648745 ... 0.14737648 0.0515794  0.24322277]\n",
      " ...\n",
      " [0.08609032 0.04878823 0.04178161 ... 0.08114466 0.14398103 0.04728632]\n",
      " [0.16517267 0.0571333  0.11995118 ... 0.21256906 0.19798698 0.1977356 ]\n",
      " [0.22855396 0.04709697 0.20192139 ... 0.08982199 0.19437528 0.02515634]]\n",
      "Hidden Layer 1, 64 neurons:\n",
      "[[0.14398331 0.03843232 0.14641213 ... 0.04085896 0.17641567 0.08207941]\n",
      " [0.18965722 0.1074977  0.15689354 ... 0.16721359 0.24035438 0.05310932]\n",
      " [0.24089148 0.08059437 0.00726509 ... 0.05275385 0.15893524 0.09752226]\n",
      " ...\n",
      " [0.1079148  0.19170164 0.11912579 ... 0.18428672 0.09861976 0.17223563]\n",
      " [0.07249806 0.19721808 0.05356178 ... 0.10347944 0.03299922 0.16208777]\n",
      " [0.1364714  0.11611272 0.04384275 ... 0.04156671 0.07470541 0.18695346]]\n",
      "Hidden Layer 2, 32 neurons:\n",
      "[[0.16879477 0.26099436 0.06034536 ... 0.19075856 0.23953843 0.13132674]\n",
      " [0.06144387 0.00829542 0.29212628 ... 0.25832691 0.32738664 0.27370326]\n",
      " [0.32923512 0.15505754 0.12494995 ... 0.30732741 0.01314431 0.36399645]\n",
      " ...\n",
      " [0.28908917 0.29855666 0.31780441 ... 0.14064491 0.26861    0.31898924]\n",
      " [0.32052869 0.22843727 0.18040166 ... 0.28949697 0.30349853 0.35045272]\n",
      " [0.07696112 0.26260903 0.13503821 ... 0.21855662 0.10401124 0.3408755 ]]\n",
      "Output Layer, 10 neurons:\n",
      "[[ 2.99792145e-01  1.31733292e-01  2.81054787e-01  3.34685416e-01\n",
      "   4.53560760e-02  5.93832582e-01  8.97840142e-02 -1.90337536e-02\n",
      "   2.93978635e-02  2.24819589e-01]\n",
      " [ 3.27235854e-01  2.12206830e-01  7.47511725e-02  2.03514324e-01\n",
      "   3.74062710e-01  3.96278832e-01  7.27749716e-02  2.24045717e-01\n",
      "   2.47980405e-01  3.84712416e-01]\n",
      " [ 1.51857011e-01  3.11944284e-01  1.30917219e-01  4.45796459e-01\n",
      "   3.85192227e-01  4.61897600e-01  2.36154400e-01  4.28530499e-01\n",
      "   4.35312341e-01  3.62110589e-01]\n",
      " [ 8.26181633e-02  1.19017212e-01  1.22757836e-01  5.45802719e-01\n",
      "   4.17065046e-01  2.99600720e-01  1.16799080e-01  3.64768291e-01\n",
      "   8.84231539e-02  1.94394031e-01]\n",
      " [ 4.61748778e-01  2.73895884e-01  1.58351080e-01  3.97460451e-01\n",
      "   5.05091593e-01  4.98381298e-01  2.41501402e-01  4.32004426e-01\n",
      "   1.43647554e-01  1.53971620e-01]\n",
      " [ 2.66906901e-01 -9.79455724e-02  5.10433249e-01  3.17665189e-01\n",
      "   1.71121427e-01  4.43394278e-01 -3.61503888e-02  1.33307326e-01\n",
      "   4.03709032e-02  4.41493705e-01]\n",
      " [ 1.38663071e-01  3.05868999e-01  1.11122656e-01  2.58047136e-01\n",
      "   3.80753673e-01  6.12074762e-01  2.42077406e-01 -2.60169898e-03\n",
      "   2.81002638e-01  5.91512217e-02]\n",
      " [ 1.67833038e-01 -3.49801634e-02  1.96778514e-01  1.24229955e-01\n",
      "   4.58235867e-02  2.71342141e-01  3.30757406e-01  3.01647703e-01\n",
      "   3.20853440e-01  2.95633888e-02]\n",
      " [ 1.27075247e-01 -1.31957483e-02  4.02518964e-01  5.45633561e-01\n",
      "   3.69306779e-01  2.20571377e-01  3.66429898e-01  1.08791525e-01\n",
      "   3.17081853e-01  1.69707115e-01]\n",
      " [-1.41945182e-02  5.61662065e-02  4.00293261e-01  3.54454561e-01\n",
      "   3.17761100e-01  6.32119113e-01 -9.11900590e-02  4.21307853e-01\n",
      "   5.63756691e-02  4.55755670e-01]\n",
      " [ 4.64131225e-01 -1.32003516e-02  9.14406547e-02  3.89085641e-01\n",
      "   4.10518074e-01  6.28643063e-01  3.60430788e-01  3.30480555e-01\n",
      "   3.63461431e-01  4.51100761e-01]\n",
      " [ 2.65476431e-01 -4.94510328e-02  2.06402885e-01  3.98207774e-01\n",
      "   4.21462480e-01  3.36531771e-01  1.66759766e-01  1.90095853e-01\n",
      "   4.02134110e-01  3.29529708e-01]\n",
      " [ 6.93360870e-02  3.15432514e-02  6.65924718e-02  3.13130577e-01\n",
      "   2.06935622e-02  4.94015274e-01  3.43406407e-01  2.35822293e-01\n",
      "   3.32006326e-01  2.41205160e-01]\n",
      " [ 3.99257705e-01  1.89321790e-01  4.72036082e-01  4.24494460e-01\n",
      "   1.70969403e-01  4.47621287e-01 -2.48120973e-02  4.70880120e-01\n",
      "   8.34069507e-02  4.30316840e-01]\n",
      " [ 3.07114854e-01  1.12186758e-01  4.02084239e-01  3.31998714e-01\n",
      "   8.35252389e-02  3.80215373e-01  2.51951130e-01  4.24378374e-01\n",
      "   1.57527336e-01  1.33485608e-02]\n",
      " [ 3.79166017e-02  2.92371102e-01  4.73841544e-01  2.44882520e-01\n",
      "   3.38353255e-01  3.75074762e-01  4.02029656e-01  1.32761801e-01\n",
      "   1.33720957e-01  3.20087813e-01]\n",
      " [ 2.06594938e-01  2.17242025e-01  4.06131030e-01  4.05625213e-01\n",
      "   2.41629093e-01  7.05774605e-01 -1.08959078e-02  3.32210856e-01\n",
      "   1.15754188e-01  4.30245068e-01]\n",
      " [ 4.30175342e-01 -1.08109947e-01  4.43344920e-01  3.75223618e-01\n",
      "   3.88880878e-01  4.30766038e-01  2.17375613e-01  3.46860532e-01\n",
      "   2.56788534e-01  1.03812579e-01]\n",
      " [ 2.66864347e-01 -5.55120130e-02  3.10370918e-01  5.60205804e-01\n",
      "   1.60783118e-01  2.93029085e-01 -3.34353926e-02  4.68916922e-01\n",
      "   3.99534672e-01  2.99122812e-01]\n",
      " [ 2.09409432e-01  1.86900526e-01  4.55639647e-02  4.82520685e-01\n",
      "   4.48973629e-01  4.22580157e-01  1.29580226e-01  4.65202580e-01\n",
      "   8.69997657e-02  3.49828261e-01]\n",
      " [ 1.00690416e-01  1.61294504e-01  7.80164713e-02  1.98549556e-01\n",
      "   2.49819886e-02  2.33801502e-01 -2.33322808e-02  2.45332695e-01\n",
      "   4.16725345e-01  1.04616752e-02]\n",
      " [ 1.39864794e-01  1.38707824e-01  2.07152023e-01  2.31046517e-01\n",
      "   3.96196726e-01  6.63030515e-01  1.09028535e-01  3.52434095e-01\n",
      "   3.99894190e-01  3.38194517e-01]\n",
      " [ 2.69542780e-01 -2.04687302e-02  2.92297107e-01  6.01049849e-01\n",
      "   3.25096025e-01  2.35654915e-01 -5.52342188e-02  2.46759341e-01\n",
      "   2.83294240e-01  3.87389612e-01]\n",
      " [ 3.17956511e-01  2.18333452e-02  3.09187798e-01  2.35813064e-01\n",
      "   4.95836774e-01  3.33305262e-01 -3.77067771e-03  3.84208134e-01\n",
      "   4.92829891e-01  6.42236144e-02]\n",
      " [ 2.05963598e-01 -8.91700543e-02  3.26715385e-01  4.04382696e-01\n",
      "   2.47529911e-01  6.26104636e-01  2.89238456e-01  2.84093543e-01\n",
      "   4.83597750e-01  3.48818216e-01]\n",
      " [ 3.24823145e-01  2.02442803e-01  2.71139507e-01  1.32439489e-01\n",
      "   1.92906580e-02  2.81233280e-01  2.17563170e-02  2.05165722e-01\n",
      "   1.77649411e-01  1.02828642e-01]\n",
      " [ 3.40908417e-01  6.81757858e-03  4.27307284e-02  1.06522038e-01\n",
      "   3.65674623e-01  3.82819464e-01  1.95123512e-01  4.58070023e-01\n",
      "   4.03260569e-01  3.55379088e-01]\n",
      " [ 3.57197895e-01  2.70192225e-03  6.77023900e-02  3.91599225e-01\n",
      "   1.95768630e-01  6.10767471e-01  3.99125165e-02  2.97725144e-01\n",
      "   4.70907268e-01  3.06347048e-01]\n",
      " [ 9.10484141e-02 -1.07695808e-01  1.80360078e-01  3.12009659e-01\n",
      "   3.36132510e-01  5.64011616e-01 -5.58155928e-02  3.93982002e-01\n",
      "   2.42222004e-01  2.95530771e-01]\n",
      " [ 3.97072639e-01  1.70970904e-01  3.27269230e-02  3.26684560e-01\n",
      "   2.59581521e-01  6.15172681e-01  3.75830695e-02  3.86087673e-01\n",
      "   2.26120113e-01  1.13510842e-01]\n",
      " [ 4.45606062e-01  2.33949087e-01  4.69539727e-01  1.67561347e-01\n",
      "   8.63416881e-02  2.32312701e-01  5.70549855e-02  4.66294513e-01\n",
      "   2.75516792e-01  1.78537655e-01]\n",
      " [ 4.13809749e-04 -7.99518962e-02  8.83997480e-02  3.71438577e-01\n",
      "   1.28045350e-01  6.27297344e-01  7.92982487e-02  5.97921715e-02\n",
      "   2.53747998e-01  7.09193192e-02]]\n",
      "output of last layer (training data)\n",
      "[[11046.23158197  3959.7715311  10971.19128224 ... 13743.62447049\n",
      "  12050.24559668 11507.18422758]\n",
      " [11160.44659603  4000.70504592 11084.60116279 ... 13885.70823183\n",
      "  12174.80192984 11626.13973492]\n",
      " [11064.8842554   3966.44618538 10989.6657244  ... 13766.78269515\n",
      "  12070.57018199 11526.57907066]\n",
      " ...\n",
      " [11042.44336788  3958.40827715 10967.39153995 ... 13738.87973501\n",
      "  12046.10543919 11503.2145425 ]\n",
      " [11155.52167971  3998.96300284 11079.70090867 ... 13879.60879096\n",
      "  12169.48105051 11621.03630263]\n",
      " [11116.52096084  3984.95289216 11040.97050651 ... 13831.03144735\n",
      "  12126.91649094 11580.37350876]]\n",
      "output of last layer (testing set):\n",
      "[[10566.1749255   3787.69536397 10494.36939221 ... 13146.34310613\n",
      "  11526.54149298 11007.08951358]\n",
      " [10590.16944724  3796.29400608 10518.16741162 ... 13176.15424867\n",
      "  11552.67735888 11032.04991514]\n",
      " [10619.17593334  3806.69318123 10547.01626382 ... 13212.28075428\n",
      "  11584.35411814 11062.30428441]\n",
      " ...\n",
      " [10610.88926093  3803.71261882 10538.78148151 ... 13201.9629341\n",
      "  11575.33542356 11053.67095028]\n",
      " [10595.0762893   3798.04837151 10523.11110686 ... 13182.30779306\n",
      "  11558.0960527  11037.20619044]\n",
      " [10601.38126993  3800.31152995 10529.32361858 ... 13190.12394522\n",
      "  11564.94100086 11043.75320471]]\n",
      "accuracy:\n",
      "0.086\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#Creates the scaffold for number of neurons and number of inputs per neuron and introduces the initialisation for weights\n",
    "class NeuralInput():\n",
    "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):#weight initialization using He Initialisation\n",
    "        self.weights = 2 * np.random.rand(number_of_inputs_per_neuron, number_of_neurons)*np.sqrt(2./number_of_inputs_per_neuron)\n",
    "        if np.any(np.isnan(self.weights)):\n",
    "            print(\"nan layer weights\")\n",
    "        else:\n",
    "            print(\"no nan present layer weights\")\n",
    "            \n",
    "#Neural network class (scaffolding for the network)\n",
    "class NeuralLayers():\n",
    "    def __init__(self, input_layer, hidden_layer1, hidden_layer2, output_layer): #establishes the number of layers in the neural network\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer1 = hidden_layer1\n",
    "        self.hidden_layer2 = hidden_layer2\n",
    "        self.output_layer = output_layer\n",
    "        \n",
    "    def ReLU(self, x): #represents the ReLU function which expresses the positive values as linear and negative inputs as zero throughout the neural network except in the last layer\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    \n",
    "    def ReLU_gradient(self, x): #computes the gradient for the application of the ReLU function at the output layer \n",
    "        return (x > 0)\n",
    "    \n",
    "    def softmax_cross_entropy(self, raw_data, labels): #computes the inputs in the last layer through the softmax cross-entropy function\n",
    "        if np.any(np.isnan(raw_data)):\n",
    "            print(\"Nan present in softmax entropy\")\n",
    "        \n",
    "        if np.any(np.abs(raw_data)>1e10):\n",
    "            print(\"logits too large\")\n",
    "        \n",
    "        raw_data = np.clip(raw_data, -1e1,1e1)\n",
    "        exp = np.exp(raw_data - np.max(raw_data))\n",
    "        \n",
    "        \n",
    "        eps=1e-10 #by adding a small values prevents the culmination of a NaN value\n",
    "        y = np.array(labels).squeeze() #turns the labels into a numpy array and then squeezes the array to remove any single dimensional entries\n",
    "        probabilities = (exp/np.sum(exp, axis=1, keepdims=True)+ eps)\n",
    "        \n",
    "        if len(raw_data) != len(labels):#making sure the output in the final layer has the same length as the corresponding labels\n",
    "            raise ValueError(\"logits length not equal to labels' length\")\n",
    "        \n",
    "        logprobabilities = (-np.log(probabilities[range(len(raw_data)), labels])+eps)\n",
    "        clipped_log = np.clip(logprobabilities,eps,1-eps)\n",
    "        loss = np.sum(logprobabilities/len(raw_data)) + eps\n",
    "        return loss, probabilities\n",
    "        print(\"this is the loss\")\n",
    "        print(loss)\n",
    "        \n",
    "    def SCE_gradient(self, probs, y_values): #computing the gradient for loss correction in the last layer of neural network\n",
    "        y_values = np.array(y_values).squeeze()\n",
    "        gradient = probs\n",
    "        gradient[np.arange(len(probs)), y_values]-=1\n",
    "        gradient = gradient/len(probs)\n",
    "        return gradient\n",
    "    \n",
    "        \n",
    "        \n",
    "    # this section trains the neural network based on the training data previously processed and cropped against the training labels\n",
    "    def train(self, training_set, training_output, iterations, learning_rate=1e-5, gradient_clip = 1.0):\n",
    "        for iteration in range(iterations):\n",
    "            output_of_input_layer, output_of_hidden_layer1, output_of_hidden_layer2, output_of_output_layer = self.contemplate(training_set)\n",
    "            \n",
    "            loss, probabilities = self.softmax_cross_entropy(output_of_output_layer, training_output)\n",
    "            output_layer_delta = self.SCE_gradient(probabilities, training_output)\n",
    "            \n",
    "            hidden_layer2_error = output_layer_delta.dot(self.output_layer.weights.T)\n",
    "            hidden_layer2_delta = hidden_layer2_error * self.ReLU_gradient(output_of_hidden_layer2)\n",
    "            \n",
    "            hidden_layer1_error = hidden_layer2_delta.dot(self.hidden_layer2.weights.T)\n",
    "            hidden_layer1_delta = hidden_layer1_error * self.ReLU_gradient(output_of_hidden_layer1)\n",
    "            \n",
    "            input_layer_error = hidden_layer1_delta.dot(self.hidden_layer1.weights.T)\n",
    "            input_layer_delta = input_layer_error * self.ReLU_gradient(output_of_input_layer)\n",
    "            #Calculation of degree of adjustment of weights\n",
    "            input_layer_weightupdate = training_set.T.dot(input_layer_delta)\n",
    "            hidden_layer1_weightupdate = output_of_input_layer.T.dot(hidden_layer1_delta)\n",
    "            hidden_layer2_weightupdate = output_of_hidden_layer1.T.dot(hidden_layer2_delta)\n",
    "            output_layer_weightupdate = output_of_hidden_layer2.T.dot(output_layer_delta)\n",
    "            \n",
    "            #adjusting the weights\n",
    "            self.input_layer.weights += learning_rate * input_layer_weightupdate\n",
    "            self.hidden_layer1.weights += learning_rate * hidden_layer1_weightupdate\n",
    "            self.hidden_layer2.weights += learning_rate * hidden_layer2_weightupdate\n",
    "            self.output_layer.weights +=  learning_rate * output_layer_weightupdate\n",
    "    \n",
    "    #passing through the data after initialisation of random weights\n",
    "    def contemplate(self, inputs):\n",
    "        output_of_input_layer = self.ReLU(np.dot(inputs, self.input_layer.weights))\n",
    "        output_of_hidden_layer1 = self.ReLU(np.dot(output_of_input_layer, self.hidden_layer1.weights))\n",
    "        output_of_hidden_layer2 = self.ReLU(np.dot(output_of_hidden_layer1, self.hidden_layer2.weights))\n",
    "        output_of_output_layer = (np.dot(output_of_hidden_layer2, self.output_layer.weights))\n",
    "        return output_of_input_layer, output_of_hidden_layer1, output_of_hidden_layer2, output_of_output_layer\n",
    "        \n",
    "    #initialisation of the weight values and subsequent passing of data and adaptive weight adjustments with each iteration\n",
    "    def weight_values(self):\n",
    "        print(\"Input Layer, 128 neurons:\")\n",
    "        print(self.input_layer.weights)\n",
    "        print(\"Hidden Layer 1, 64 neurons:\")\n",
    "        print(self.hidden_layer1.weights)\n",
    "        print(\"Hidden Layer 2, 32 neurons:\")\n",
    "        print(self.hidden_layer2.weights)\n",
    "        print(\"Output Layer, 10 neurons:\")\n",
    "        print(self.output_layer.weights)\n",
    "    \n",
    "    #provides the final output of the neural network through the the last layer\n",
    "    def output_values(self, input):\n",
    "        print(\"output labels\")\n",
    "        print(self.contemplate(input)[-1])\n",
    "    \n",
    "def accuracy_measurement(output_values, pred_output):\n",
    "    op = np.argmax(output_values, axis = 1)\n",
    "    return np.mean(op == pred_output)\n",
    "\n",
    "\n",
    "        \n",
    "random.seed(1)\n",
    "#assigning the datasets to values\n",
    "training_set = x_train_shortened\n",
    "training_output = y_train_shortened\n",
    "testing_set = x_test_shortened\n",
    "testing_output = y_test_shortened\n",
    "\n",
    "#contruction of the neural network with the number of neurons with the number of inputs per neuron\n",
    "input_layer = NeuralInput(128, len(training_set[1]))\n",
    "hidden_layer1 = NeuralInput(64, 128)\n",
    "hidden_layer2 = NeuralInput(32, 64)\n",
    "output_layer = NeuralInput(10, 32)\n",
    "\n",
    "#strucutally connection each layer \n",
    "NN = NeuralLayers(input_layer, hidden_layer1, hidden_layer2, output_layer)\n",
    "\n",
    "\n",
    "#training of model\n",
    "NN.train(training_set, training_output, 1000)\n",
    "\n",
    "#initialisation of random weights\n",
    "NN.weight_values()\n",
    "\n",
    "#adjustment of weights after passing of data\n",
    "print(\"weights after training:\")\n",
    "NN.weight_values()\n",
    "\n",
    "#prints the output predicted labels for the training set\n",
    "print(\"output of last layer (training data)\")\n",
    "_,_,_,output = NN.contemplate(training_set)\n",
    "print(output)\n",
    "\n",
    "#prints the output of the predicted labels for the testing set\n",
    "print(\"output of last layer (testing set):\")\n",
    "_,_,_,output_values = NN.contemplate(testing_set)\n",
    "print(output_values)\n",
    "#prints the accuracy of the predicted labels against the actual labels \n",
    "print(\"accuracy:\")\n",
    "print(accuracy_measurement(output_values, testing_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd82946",
   "metadata": {},
   "source": [
    "In the below code a weight decay constant was added to see how it would affect the neural network's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b16a5d",
   "metadata": {},
   "source": [
    "Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4563578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no nan present layer weights\n",
      "no nan present layer weights\n",
      "no nan present layer weights\n",
      "no nan present layer weights\n",
      "Input Layer, 128 neurons:\n",
      "[[0.21550567 0.02991575 0.13238159 ... 0.0172205  0.16853352 0.13656146]\n",
      " [0.23237197 0.16634958 0.22025187 ... 0.16492691 0.17541121 0.06256828]\n",
      " [0.23385592 0.18822666 0.05537896 ... 0.07775651 0.17562529 0.01828559]\n",
      " ...\n",
      " [0.12097666 0.04259515 0.21843538 ... 0.13426466 0.07760648 0.04093293]\n",
      " [0.14159543 0.03899825 0.14408542 ... 0.22581267 0.18080456 0.12570971]\n",
      " [0.1393596  0.14188175 0.1502947  ... 0.02689995 0.06037896 0.10083597]]\n",
      "Hidden Layer 1, 64 neurons:\n",
      "[[0.16713914 0.23573274 0.13817096 ... 0.18290573 0.22185216 0.06374025]\n",
      " [0.05790557 0.00368067 0.11182265 ... 0.22585954 0.07431722 0.11693311]\n",
      " [0.23005656 0.13300784 0.09532513 ... 0.012394   0.03797439 0.16329582]\n",
      " ...\n",
      " [0.01149962 0.23671702 0.03964989 ... 0.07535287 0.10853224 0.08850048]\n",
      " [0.01994977 0.02405198 0.1298514  ... 0.10972172 0.12231171 0.19821658]\n",
      " [0.20595752 0.00376111 0.10250275 ... 0.04307841 0.07739726 0.17290116]]\n",
      "Hidden Layer 2, 32 neurons:\n",
      "[[0.09691222 0.03341766 0.27158904 ... 0.19184256 0.02739775 0.25688053]\n",
      " [0.02752676 0.33020919 0.09153781 ... 0.30546182 0.06540033 0.31847847]\n",
      " [0.08237902 0.03909658 0.09008606 ... 0.03616058 0.27020396 0.32522136]\n",
      " ...\n",
      " [0.2910298  0.33549777 0.02912499 ... 0.23413502 0.14776032 0.30670395]\n",
      " [0.16796834 0.13837379 0.20988871 ... 0.15927831 0.11067737 0.04707954]\n",
      " [0.31013211 0.17279384 0.02323562 ... 0.1184521  0.14238921 0.33027438]]\n",
      "Output Layer, 10 neurons:\n",
      "[[0.21443106 0.34095225 0.16972175 0.3567112  0.00841292 0.12732053\n",
      "  0.16989756 0.2070165  0.34117127 0.07635041]\n",
      " [0.29675552 0.20503698 0.1566053  0.04372423 0.01105819 0.45798003\n",
      "  0.30920421 0.49231305 0.43708217 0.3033648 ]\n",
      " [0.21631404 0.2673708  0.06669304 0.40800835 0.28966978 0.33751479\n",
      "  0.11693641 0.27193938 0.25182294 0.45177889]\n",
      " [0.22308339 0.25775383 0.26636361 0.1772362  0.29935451 0.38667788\n",
      "  0.45964802 0.4063658  0.17687277 0.20439292]\n",
      " [0.28414406 0.151665   0.05033094 0.38200803 0.19786794 0.20688922\n",
      "  0.45869711 0.01243471 0.18322191 0.30563171]\n",
      " [0.11383678 0.05413351 0.22476417 0.43010364 0.2059496  0.10351113\n",
      "  0.21040781 0.18376255 0.36656089 0.26590582]\n",
      " [0.1943418  0.41493245 0.0512126  0.09550423 0.18323404 0.27126673\n",
      "  0.31517964 0.44398622 0.20443559 0.00616435]\n",
      " [0.28127232 0.10788576 0.15818111 0.16625533 0.25914639 0.0242928\n",
      "  0.00144641 0.32430026 0.38577885 0.09538881]\n",
      " [0.21375166 0.21071978 0.09462729 0.33566669 0.44909371 0.0522356\n",
      "  0.02109238 0.27849019 0.36919327 0.01108163]\n",
      " [0.29663518 0.05349941 0.09341244 0.39199448 0.48594416 0.30238927\n",
      "  0.09620257 0.41148087 0.02598338 0.06437825]\n",
      " [0.12488741 0.37887363 0.19672461 0.22259702 0.10321665 0.43181021\n",
      "  0.1518071  0.49405257 0.05282862 0.49069512]\n",
      " [0.38355398 0.40613884 0.4347467  0.13643816 0.15259848 0.30695522\n",
      "  0.24546903 0.2326449  0.3080773  0.49858464]\n",
      " [0.38348275 0.49300995 0.23735238 0.23392329 0.37709644 0.22039433\n",
      "  0.4995387  0.47140521 0.35656297 0.0810134 ]\n",
      " [0.17669155 0.11744845 0.35327867 0.40508505 0.20508689 0.08711395\n",
      "  0.04827996 0.09596486 0.2935216  0.33165031]\n",
      " [0.45207313 0.45120043 0.16338661 0.35226945 0.32717344 0.14075497\n",
      "  0.17380898 0.33318607 0.04237581 0.33498862]\n",
      " [0.12414906 0.12063376 0.11094093 0.15952068 0.12062252 0.38640614\n",
      "  0.0616124  0.30904657 0.20918865 0.43488323]\n",
      " [0.16275532 0.32928975 0.16834017 0.03813266 0.03509271 0.05284703\n",
      "  0.36780161 0.09287304 0.29951958 0.31278756]\n",
      " [0.33069173 0.36753122 0.30420117 0.23317019 0.47949539 0.45722578\n",
      "  0.2782118  0.32356652 0.37412583 0.31497557]\n",
      " [0.16061417 0.42968903 0.20175782 0.03086811 0.21769157 0.49514142\n",
      "  0.2342638  0.26177089 0.25092344 0.03085667]\n",
      " [0.13361348 0.41461349 0.36768867 0.36671741 0.25205626 0.26187537\n",
      "  0.31633024 0.24868832 0.09072589 0.1375114 ]\n",
      " [0.25139709 0.04275416 0.13882334 0.35423954 0.03925414 0.01709359\n",
      "  0.31863446 0.15000905 0.0898824  0.23147012]\n",
      " [0.36874023 0.28738692 0.33896704 0.04759715 0.13758707 0.06766115\n",
      "  0.03746693 0.44969044 0.4477247  0.21016189]\n",
      " [0.05651074 0.13277536 0.40044504 0.14352402 0.39033121 0.35046337\n",
      "  0.19921278 0.30005349 0.45642972 0.18368496]\n",
      " [0.16021091 0.34283854 0.4874001  0.49873328 0.03830111 0.34714151\n",
      "  0.13472348 0.06727325 0.24803613 0.24160605]\n",
      " [0.05790544 0.12880516 0.47595623 0.2156153  0.08371826 0.43902542\n",
      "  0.12760018 0.47466841 0.12275259 0.00472961]\n",
      " [0.34767251 0.09785312 0.46855848 0.02882906 0.19066198 0.01004871\n",
      "  0.25812572 0.28547749 0.26282084 0.32492497]\n",
      " [0.11746415 0.012483   0.30066367 0.23178788 0.05191536 0.36691576\n",
      "  0.16712907 0.37024598 0.33857069 0.26479023]\n",
      " [0.22845952 0.0578115  0.49586552 0.03954779 0.11101334 0.29774841\n",
      "  0.27317065 0.04472876 0.10619522 0.3385397 ]\n",
      " [0.21641122 0.20974807 0.47626795 0.0744527  0.23722269 0.41611329\n",
      "  0.09960529 0.06344922 0.49487625 0.08681965]\n",
      " [0.2248221  0.38572431 0.33832371 0.07419407 0.04496319 0.02446491\n",
      "  0.26947545 0.21059342 0.4299873  0.0625563 ]\n",
      " [0.49332164 0.36216457 0.4288336  0.0503219  0.44996228 0.19026822\n",
      "  0.00718439 0.36757671 0.4929842  0.05441064]\n",
      " [0.253427   0.40428498 0.03493237 0.39243378 0.30673304 0.24795869\n",
      "  0.1789511  0.27787152 0.25186877 0.31211007]]\n",
      "weights after training:\n",
      "Input Layer, 128 neurons:\n",
      "[[0.21550567 0.02991575 0.13238159 ... 0.0172205  0.16853352 0.13656146]\n",
      " [0.23237197 0.16634958 0.22025187 ... 0.16492691 0.17541121 0.06256828]\n",
      " [0.23385592 0.18822666 0.05537896 ... 0.07775651 0.17562529 0.01828559]\n",
      " ...\n",
      " [0.12097666 0.04259515 0.21843538 ... 0.13426466 0.07760648 0.04093293]\n",
      " [0.14159543 0.03899825 0.14408542 ... 0.22581267 0.18080456 0.12570971]\n",
      " [0.1393596  0.14188175 0.1502947  ... 0.02689995 0.06037896 0.10083597]]\n",
      "Hidden Layer 1, 64 neurons:\n",
      "[[0.16713914 0.23573274 0.13817096 ... 0.18290573 0.22185216 0.06374025]\n",
      " [0.05790557 0.00368067 0.11182265 ... 0.22585954 0.07431722 0.11693311]\n",
      " [0.23005656 0.13300784 0.09532513 ... 0.012394   0.03797439 0.16329582]\n",
      " ...\n",
      " [0.01149962 0.23671702 0.03964989 ... 0.07535287 0.10853224 0.08850048]\n",
      " [0.01994977 0.02405198 0.1298514  ... 0.10972172 0.12231171 0.19821658]\n",
      " [0.20595752 0.00376111 0.10250275 ... 0.04307841 0.07739726 0.17290116]]\n",
      "Hidden Layer 2, 32 neurons:\n",
      "[[0.09691222 0.03341766 0.27158904 ... 0.19184256 0.02739775 0.25688053]\n",
      " [0.02752676 0.33020919 0.09153781 ... 0.30546182 0.06540033 0.31847847]\n",
      " [0.08237902 0.03909658 0.09008606 ... 0.03616058 0.27020396 0.32522136]\n",
      " ...\n",
      " [0.2910298  0.33549777 0.02912499 ... 0.23413502 0.14776032 0.30670395]\n",
      " [0.16796834 0.13837379 0.20988871 ... 0.15927831 0.11067737 0.04707954]\n",
      " [0.31013211 0.17279384 0.02323562 ... 0.1184521  0.14238921 0.33027438]]\n",
      "Output Layer, 10 neurons:\n",
      "[[0.21443106 0.34095225 0.16972175 0.3567112  0.00841292 0.12732053\n",
      "  0.16989756 0.2070165  0.34117127 0.07635041]\n",
      " [0.29675552 0.20503698 0.1566053  0.04372423 0.01105819 0.45798003\n",
      "  0.30920421 0.49231305 0.43708217 0.3033648 ]\n",
      " [0.21631404 0.2673708  0.06669304 0.40800835 0.28966978 0.33751479\n",
      "  0.11693641 0.27193938 0.25182294 0.45177889]\n",
      " [0.22308339 0.25775383 0.26636361 0.1772362  0.29935451 0.38667788\n",
      "  0.45964802 0.4063658  0.17687277 0.20439292]\n",
      " [0.28414406 0.151665   0.05033094 0.38200803 0.19786794 0.20688922\n",
      "  0.45869711 0.01243471 0.18322191 0.30563171]\n",
      " [0.11383678 0.05413351 0.22476417 0.43010364 0.2059496  0.10351113\n",
      "  0.21040781 0.18376255 0.36656089 0.26590582]\n",
      " [0.1943418  0.41493245 0.0512126  0.09550423 0.18323404 0.27126673\n",
      "  0.31517964 0.44398622 0.20443559 0.00616435]\n",
      " [0.28127232 0.10788576 0.15818111 0.16625533 0.25914639 0.0242928\n",
      "  0.00144641 0.32430026 0.38577885 0.09538881]\n",
      " [0.21375166 0.21071978 0.09462729 0.33566669 0.44909371 0.0522356\n",
      "  0.02109238 0.27849019 0.36919327 0.01108163]\n",
      " [0.29663518 0.05349941 0.09341244 0.39199448 0.48594416 0.30238927\n",
      "  0.09620257 0.41148087 0.02598338 0.06437825]\n",
      " [0.12488741 0.37887363 0.19672461 0.22259702 0.10321665 0.43181021\n",
      "  0.1518071  0.49405257 0.05282862 0.49069512]\n",
      " [0.38355398 0.40613884 0.4347467  0.13643816 0.15259848 0.30695522\n",
      "  0.24546903 0.2326449  0.3080773  0.49858464]\n",
      " [0.38348275 0.49300995 0.23735238 0.23392329 0.37709644 0.22039433\n",
      "  0.4995387  0.47140521 0.35656297 0.0810134 ]\n",
      " [0.17669155 0.11744845 0.35327867 0.40508505 0.20508689 0.08711395\n",
      "  0.04827996 0.09596486 0.2935216  0.33165031]\n",
      " [0.45207313 0.45120043 0.16338661 0.35226945 0.32717344 0.14075497\n",
      "  0.17380898 0.33318607 0.04237581 0.33498862]\n",
      " [0.12414906 0.12063376 0.11094093 0.15952068 0.12062252 0.38640614\n",
      "  0.0616124  0.30904657 0.20918865 0.43488323]\n",
      " [0.16275532 0.32928975 0.16834017 0.03813266 0.03509271 0.05284703\n",
      "  0.36780161 0.09287304 0.29951958 0.31278756]\n",
      " [0.33069173 0.36753122 0.30420117 0.23317019 0.47949539 0.45722578\n",
      "  0.2782118  0.32356652 0.37412583 0.31497557]\n",
      " [0.16061417 0.42968903 0.20175782 0.03086811 0.21769157 0.49514142\n",
      "  0.2342638  0.26177089 0.25092344 0.03085667]\n",
      " [0.13361348 0.41461349 0.36768867 0.36671741 0.25205626 0.26187537\n",
      "  0.31633024 0.24868832 0.09072589 0.1375114 ]\n",
      " [0.25139709 0.04275416 0.13882334 0.35423954 0.03925414 0.01709359\n",
      "  0.31863446 0.15000905 0.0898824  0.23147012]\n",
      " [0.36874023 0.28738692 0.33896704 0.04759715 0.13758707 0.06766115\n",
      "  0.03746693 0.44969044 0.4477247  0.21016189]\n",
      " [0.05651074 0.13277536 0.40044504 0.14352402 0.39033121 0.35046337\n",
      "  0.19921278 0.30005349 0.45642972 0.18368496]\n",
      " [0.16021091 0.34283854 0.4874001  0.49873328 0.03830111 0.34714151\n",
      "  0.13472348 0.06727325 0.24803613 0.24160605]\n",
      " [0.05790544 0.12880516 0.47595623 0.2156153  0.08371826 0.43902542\n",
      "  0.12760018 0.47466841 0.12275259 0.00472961]\n",
      " [0.34767251 0.09785312 0.46855848 0.02882906 0.19066198 0.01004871\n",
      "  0.25812572 0.28547749 0.26282084 0.32492497]\n",
      " [0.11746415 0.012483   0.30066367 0.23178788 0.05191536 0.36691576\n",
      "  0.16712907 0.37024598 0.33857069 0.26479023]\n",
      " [0.22845952 0.0578115  0.49586552 0.03954779 0.11101334 0.29774841\n",
      "  0.27317065 0.04472876 0.10619522 0.3385397 ]\n",
      " [0.21641122 0.20974807 0.47626795 0.0744527  0.23722269 0.41611329\n",
      "  0.09960529 0.06344922 0.49487625 0.08681965]\n",
      " [0.2248221  0.38572431 0.33832371 0.07419407 0.04496319 0.02446491\n",
      "  0.26947545 0.21059342 0.4299873  0.0625563 ]\n",
      " [0.49332164 0.36216457 0.4288336  0.0503219  0.44996228 0.19026822\n",
      "  0.00718439 0.36757671 0.4929842  0.05441064]\n",
      " [0.253427   0.40428498 0.03493237 0.39243378 0.30673304 0.24795869\n",
      "  0.1789511  0.27787152 0.25186877 0.31211007]]\n",
      "output of last layer (training data)\n",
      "[[10312.26122364 10879.12428963 11154.63212459 ... 12171.7577819\n",
      "  11974.25026958  9620.84985192]\n",
      " [10423.2489879  10996.19097322 11274.69676519 ... 12302.76254662\n",
      "  12103.10456648  9724.40139335]\n",
      " [10327.79195458 10895.51453815 11171.45163101 ... 12190.10980512\n",
      "  11992.28054562  9635.3404385 ]\n",
      " ...\n",
      " [10307.08084585 10873.65359318 11149.02404149 ... 12165.63686361\n",
      "  11968.22883159  9616.01525416]\n",
      " [10413.16588037 10985.58394776 11263.80664772 ... 12290.87636878\n",
      "  12091.45331057  9715.00468072]\n",
      " [10384.45344653 10955.29369335 11232.71306213 ... 12256.96591116\n",
      "  12058.07272583  9688.17826135]]\n",
      "output of last layer (testing set):\n",
      "[[ 9865.81175756 10408.11623265 10671.69578731 ... 11644.79076022\n",
      "  11455.81909868  9204.30848103]\n",
      " [ 9886.74302652 10430.22446374 10694.38402828 ... 11669.53335721\n",
      "  11480.15289399  9223.88672222]\n",
      " [ 9914.12686713 10459.10285513 10723.99033743 ... 11701.84286036\n",
      "  11511.93070498  9249.4115344 ]\n",
      " ...\n",
      " [ 9910.0614315  10454.81330034 10719.57710641 ... 11697.01894521\n",
      "  11507.21343689  9245.59744902]\n",
      " [ 9884.63481776 10427.9827069  10692.07434016 ... 11667.02808228\n",
      "  11477.71605275  9221.89028426]\n",
      " [ 9896.46153415 10440.47006646 10704.89245411 ... 11680.9968777\n",
      "  11491.45603167  9232.93616167]]\n",
      "accuracy:\n",
      "0.102\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#Creates the scaffold for number of neurons and number of inputs per neuron and introduces the initialisation for weights\n",
    "class NeuralInput():\n",
    "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):#weight initialization using He Initialisation\n",
    "        self.weights = 2 * np.random.rand(number_of_inputs_per_neuron, number_of_neurons)*np.sqrt(2./number_of_inputs_per_neuron)\n",
    "        if np.any(np.isnan(self.weights)):\n",
    "            print(\"nan layer weights\")\n",
    "        else:\n",
    "            print(\"no nan present layer weights\")\n",
    "            \n",
    "#Neural network class (scaffolding for the network)\n",
    "class NeuralLayers():\n",
    "    def __init__(self, input_layer, hidden_layer1, hidden_layer2, output_layer): #establishes the number of layers in the neural network\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer1 = hidden_layer1\n",
    "        self.hidden_layer2 = hidden_layer2\n",
    "        self.output_layer = output_layer\n",
    "        \n",
    "    def batch_normalisation (self, data, constant = 1e-5):#batch normalisation addition for data \n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_variance = np.var(data, axis=0)\n",
    "        \n",
    "        data_normalised = (data - data_mean)/np.sqrt(data_variance - constant)\n",
    "        \n",
    "        return data_normalised\n",
    "    \n",
    "        \n",
    "    def ReLU(self, x): #represents the ReLU function which expresses the positive values as linear and negative inputs as zero throughout the neural network except in the last layer\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    \n",
    "    def ReLU_gradient(self, x): #computes the gradient for the application of the ReLU function at the output layer \n",
    "        return (x > 0)\n",
    "    \n",
    "    def softmax_cross_entropy(self, raw_data, labels): #computes the inputs in the last layer through the softmax cross-entropy function\n",
    "        if np.any(np.isnan(raw_data)):\n",
    "            print(\"Nan present in softmax entropy\")\n",
    "        \n",
    "        if np.any(np.abs(raw_data)>1e10):\n",
    "            print(\"logits too large\")\n",
    "        \n",
    "        raw_data = np.clip(raw_data, -1e1,1e1)\n",
    "        exp = np.exp(raw_data - np.max(raw_data))\n",
    "        \n",
    "        \n",
    "        eps=1e-10 #by adding a small values prevents the culmination of a NaN value\n",
    "        y = np.array(labels).squeeze() #turns the labels into a numpy array and then squeezes the array to remove any single dimensional entries\n",
    "        probabilities = (exp/np.sum(exp, axis=1, keepdims=True)+ eps)\n",
    "        \n",
    "        if len(raw_data) != len(labels):#making sure the output in the final layer has the same length as the corresponding labels\n",
    "            raise ValueError(\"logits length not equal to labels' length\")\n",
    "        \n",
    "        logprobabilities = (-np.log(probabilities[range(len(raw_data)), labels])+eps)\n",
    "        clipped_log = np.clip(logprobabilities,eps,1-eps)\n",
    "        loss = np.sum(logprobabilities/len(raw_data)) + eps\n",
    "        return loss, probabilities\n",
    "        print(\"this is the loss\")\n",
    "        print(loss)\n",
    "        \n",
    "    def SCE_gradient(self, probs, y_values, learning_rate = 1e-8): #computing the gradient for loss correction in the last layer of neural network in a momentum stochastic gradient descent manner with momentum value\n",
    "        y_values = np.array(y_values).squeeze()\n",
    "        size = len(y_values)\n",
    "        gradient = probs\n",
    "        gradient[np.arange(size), y_values]-=1\n",
    "        gradient = gradient/size\n",
    "        return gradient\n",
    "    \n",
    "        \n",
    "        \n",
    "    # this section trains the neural network based on the training data previously processed and cropped against the training labels\n",
    "    def train(self, training_set, training_output, iterations, learning_rate=1e-8, gradient_clip = 1.0, wd_constant=0.01, momentum = 0.9):\n",
    "        for iteration in range(iterations):\n",
    "            output_of_input_layer, output_of_hidden_layer1, output_of_hidden_layer2, output_of_output_layer = self.contemplate(training_set)\n",
    "            \n",
    "            output_of_input_layer = self.batch_normalisation(output_of_input_layer)\n",
    "            output_of_hidden_layer1 = self.batch_normalisation(output_of_hidden_layer1)\n",
    "            output_of_hidden_layer2 = self.batch_normalisation(output_of_hidden_layer2)\n",
    "            \n",
    "            loss, probabilities = self.softmax_cross_entropy(output_of_output_layer, training_output)\n",
    "            output_layer_delta = self.SCE_gradient(probabilities, training_output)\n",
    "            \n",
    "            hidden_layer2_error = output_layer_delta.dot(self.output_layer.weights.T)\n",
    "            hidden_layer2_delta = hidden_layer2_error * self.ReLU_gradient(output_of_hidden_layer2)\n",
    "            \n",
    "            hidden_layer1_error = hidden_layer2_delta.dot(self.hidden_layer2.weights.T)\n",
    "            hidden_layer1_delta = hidden_layer1_error * self.ReLU_gradient(output_of_hidden_layer1)\n",
    "            \n",
    "            input_layer_error = hidden_layer1_delta.dot(self.hidden_layer1.weights.T)\n",
    "            input_layer_delta = input_layer_error * self.ReLU_gradient(output_of_input_layer)\n",
    "            #Calculation of degree of adjustment of weights\n",
    "            input_layer_weightupdate = training_set.T.dot(input_layer_delta)\n",
    "            hidden_layer1_weightupdate = output_of_input_layer.T.dot(hidden_layer1_delta)\n",
    "            hidden_layer2_weightupdate = output_of_hidden_layer1.T.dot(hidden_layer2_delta)\n",
    "            output_layer_weightupdate = output_of_hidden_layer2.T.dot(output_layer_delta)\n",
    "            \n",
    "            #adjusting the weights\n",
    "            self.input_layer.weights += (learning_rate * input_layer_weightupdate-(learning_rate*wd_constant*self.input_layer.weights))*momentum\n",
    "            self.hidden_layer1.weights += (learning_rate * hidden_layer1_weightupdate-(learning_rate*wd_constant*self.hidden_layer1.weights))*momentum\n",
    "            self.hidden_layer2.weights += (learning_rate * hidden_layer2_weightupdate-(learning_rate*wd_constant*self.hidden_layer2.weights))*momentum\n",
    "            self.output_layer.weights +=  (learning_rate * output_layer_weightupdate-(learning_rate*wd_constant*self.output_layer.weights))*momentum\n",
    "    \n",
    "    #passing through the data after initialisation of random weights\n",
    "    def contemplate(self, inputs):\n",
    "        output_of_input_layer = self.ReLU(np.dot(inputs, self.input_layer.weights))\n",
    "        output_of_hidden_layer1 = self.ReLU(np.dot(output_of_input_layer, self.hidden_layer1.weights))\n",
    "        output_of_hidden_layer2 = self.ReLU(np.dot(output_of_hidden_layer1, self.hidden_layer2.weights))\n",
    "        output_of_output_layer = (np.dot(output_of_hidden_layer2, self.output_layer.weights))\n",
    "        return output_of_input_layer, output_of_hidden_layer1, output_of_hidden_layer2, output_of_output_layer\n",
    "        \n",
    "    #initialisation of the weight values and subsequent passing of data and adaptive weight adjustments with each iteration\n",
    "    def weight_values(self):\n",
    "        print(\"Input Layer, 128 neurons:\")\n",
    "        print(self.input_layer.weights)\n",
    "        print(\"Hidden Layer 1, 64 neurons:\")\n",
    "        print(self.hidden_layer1.weights)\n",
    "        print(\"Hidden Layer 2, 32 neurons:\")\n",
    "        print(self.hidden_layer2.weights)\n",
    "        print(\"Output Layer, 10 neurons:\")\n",
    "        print(self.output_layer.weights)\n",
    "    \n",
    "    #provides the final output of the neural network through the the last layer\n",
    "    def output_values(self, input):\n",
    "        print(\"output labels\")\n",
    "        print(self.contemplate(input)[-1])\n",
    "    \n",
    "def accuracy_measurement(output_values, pred_output):\n",
    "    op = np.argmax(output_values, axis = 1)\n",
    "    return np.mean(op == pred_output)\n",
    "\n",
    "\n",
    "        \n",
    "random.seed(1)\n",
    "#assigning the datasets to values\n",
    "training_set = x_train_shortened\n",
    "training_output = y_train_shortened\n",
    "testing_set = x_test_shortened\n",
    "testing_output = y_test_shortened\n",
    "\n",
    "#contruction of the neural network with the number of neurons with the number of inputs per neuron\n",
    "input_layer = NeuralInput(128, len(training_set[1]))\n",
    "hidden_layer1 = NeuralInput(64, 128)\n",
    "hidden_layer2 = NeuralInput(32, 64)\n",
    "output_layer = NeuralInput(10, 32)\n",
    "\n",
    "#strucutally connecting each layer \n",
    "NN = NeuralLayers(input_layer, hidden_layer1, hidden_layer2, output_layer)\n",
    "\n",
    "\n",
    "#training of model\n",
    "NN.train(training_set, training_output, 1000)\n",
    "\n",
    "#initialisation of random weights\n",
    "NN.weight_values()\n",
    "\n",
    "#adjustment of weights after passing of data\n",
    "print(\"weights after training:\")\n",
    "NN.weight_values()\n",
    "\n",
    "#prints the output predicted labels for the training set\n",
    "print(\"output of last layer (training data)\")\n",
    "_,_,_,output = NN.contemplate(training_set)\n",
    "print(output)\n",
    "\n",
    "#prints the output of the predicted labels for the testing set\n",
    "print(\"output of last layer (testing set):\")\n",
    "_,_,_,output_values = NN.contemplate(testing_set)\n",
    "print(output_values)\n",
    "#prints the accuracy of the predicted labels against the actual labels \n",
    "print(\"accuracy:\")\n",
    "print(accuracy_measurement(output_values, testing_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fb79b",
   "metadata": {},
   "source": [
    "Section 4 - measurements pertain to section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dfb75b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤════════════════════════╕\n",
      "│   iterations │   accuracy measurement │\n",
      "╞══════════════╪════════════════════════╡\n",
      "│         1000 │                  0.086 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│          950 │                  0.086 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│          900 │                  0.086 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│          500 │                  0.086 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│         1500 │                  0.086 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│        10000 │                  0.103 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│         7500 │                  0.103 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│         5000 │                  0.103 │\n",
      "╘══════════════╧════════════════════════╛\n",
      "This measurement was conducted with a constant learning rate of 1e-5, at higher iterations it was found that NaN values started to dominate the neural layers\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate \n",
    "data = [[1000,0.086], [950,0.086] , [900, 0.086] , [500,0.086], [1500,0.086], [10000, 0.103], [7500, 0.103], [5000, 0.103]]\n",
    "#data = [Iterations, accuracy]\n",
    "\n",
    "col_names = [\"iterations\", \"accuracy measurement\"]\n",
    "\n",
    "table = tabulate(data, headers=col_names, tablefmt=\"fancy_grid\")\n",
    "\n",
    "print(table)\n",
    "print(\"This measurement was conducted with a constant learning rate of 1e-5, at higher iterations it was found that NaN values started to dominate the neural layers, only conducted over 1 epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46712bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this measurement was conducted over a range of learning rates with the number of iterations keep constant at 1000, the accuracy was averaged over 3 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LR_Acc = [[1e-4, 0.103], [1e-5,0.086], [1e-6,0.097], [1e-7, 0.099], [1e-8,0.093], [1e-9, 0.096], [1e-10, 0.096]]\n",
    "\n",
    "xx=np.array([x[0] for x in LR_Acc])\n",
    "yy=np.array([x[1] for x in LR_Acc])\n",
    "\n",
    "\n",
    "plt.scatter(xx, yy)\n",
    "plt.xlabel(\"leaning rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning rate vs Accuracy\")\n",
    "plt.xlim([-0.00001,0.00002])\n",
    "print(\"this measurement was conducted over a range of learning rates with the number of iterations keep constant at 1000, the accuracy was averaged over 3 epochs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0398c",
   "metadata": {},
   "source": [
    "Section 5 - measurements pertain to section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc2806ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤════════════════════════╕\n",
      "│   iterations │   accuracy measurement │\n",
      "╞══════════════╪════════════════════════╡\n",
      "│         1000 │                  0.09  │\n",
      "├──────────────┼────────────────────────┤\n",
      "│          950 │                  0.109 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│          500 │                  0.103 │\n",
      "├──────────────┼────────────────────────┤\n",
      "│        10000 │                  0.1   │\n",
      "╘══════════════╧════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate \n",
    "iteration_accuracy = [[1000,0.09], [950,0.109], [500,0.103], [10000,0.100]]\n",
    "\n",
    "col_names = [\"iterations\", \"accuracy measurement\"]\n",
    "\n",
    "table = tabulate(iteration_accuracy, headers=col_names, tablefmt=\"fancy_grid\")\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9af0683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1e-05, 2e-05)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learning_rate = 1e-5, 1e-4, 1e-6, 1e-7, 1e-8\n",
    "accuracy = 0.106, 0.093, 0.1, 0.105, 0.094\n",
    "\n",
    "plt.scatter(Learning_rate, accuracy)\n",
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Learning rate vs Accuracy\")\n",
    "plt.xlim([-0.00001,0.00002])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
